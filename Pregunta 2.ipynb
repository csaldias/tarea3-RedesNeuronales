{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pregunta 2 T3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "8k6fptbsWLru",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pregunta 2 - Question Answering"
      ]
    },
    {
      "metadata": {
        "id": "wy7HyPVbOKvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Parte a)**"
      ]
    },
    {
      "metadata": {
        "id": "QQC7mIRvY3cV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Cargamos los ejemplos del dataset SQuAD 2.0 (The Stanford Question Answering Dataset):"
      ]
    },
    {
      "metadata": {
        "id": "qfeImSq3Nv1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b6f99362-3ef6-4658-9cd9-21f7dc655aa7"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "link_train = 'https://raw.githubusercontent.com/csaldias/tarea3-RedesNeuronales/master/train_Q-A.csv'\n",
        "link_test  = 'https://raw.githubusercontent.com/csaldias/tarea3-RedesNeuronales/master/test_Q.csv'\n",
        "\n",
        "df_train = pd.read_csv(link_train)\n",
        "df_train.dropna(inplace=True)\n",
        "df_test = pd.read_csv(link_test)\n",
        "df_train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>Houston, Texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         id  \\\n",
              "0  56be85543aeaaa14008c9063   \n",
              "1  56be85543aeaaa14008c9065   \n",
              "2  56be85543aeaaa14008c9066   \n",
              "3  56bf6b0f3aeaaa14008c9601   \n",
              "4  56bf6b0f3aeaaa14008c9602   \n",
              "\n",
              "                                            question               answer  \n",
              "0           When did Beyonce start becoming popular?    in the late 1990s  \n",
              "1  What areas did Beyonce compete in when she was...  singing and dancing  \n",
              "2  When did Beyonce leave Destiny's Child and bec...                 2003  \n",
              "3      In what city and state did Beyonce  grow up?        Houston, Texas  \n",
              "4         In which decade did Beyonce become famous?           late 1990s  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "kn70nGTNZkL0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "El set de entrenamiento consiste en una serie de pares pregunta-respuesta, cada una indentificada con un ID único. Tanto las preguntas como las respuestas son una serie de caracteres alfanuméricos, con preguntas y respuestas en lenguaje natural."
      ]
    },
    {
      "metadata": {
        "id": "VxQ8wKIUTTa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1b24c9f6-d59a-4206-c1f4-8b607083b88e"
      },
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56ddde6b9a695914005b9628</td>\n",
              "      <td>In what country is Normandy located?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56ddde6b9a695914005b9629</td>\n",
              "      <td>When were the Normans in Normandy?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56ddde6b9a695914005b962a</td>\n",
              "      <td>From which countries did the Norse originate?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56ddde6b9a695914005b962b</td>\n",
              "      <td>Who was the Norse leader?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56ddde6b9a695914005b962c</td>\n",
              "      <td>What century did the Normans first gain their ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         id                                           question\n",
              "0  56ddde6b9a695914005b9628               In what country is Normandy located?\n",
              "1  56ddde6b9a695914005b9629                 When were the Normans in Normandy?\n",
              "2  56ddde6b9a695914005b962a      From which countries did the Norse originate?\n",
              "3  56ddde6b9a695914005b962b                          Who was the Norse leader?\n",
              "4  56ddde6b9a695914005b962c  What century did the Normans first gain their ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Is5SkVekbgHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "El set de pruebas consiste en una serie de preguntas con las que será posible medir el desempeño de nuestra red, cada una acompañada de un ID único."
      ]
    },
    {
      "metadata": {
        "id": "n4fvKeaVPNxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "46c31800-d391-4561-c84a-33252a36552e"
      },
      "cell_type": "code",
      "source": [
        "print(\"Hay {} ejemplos disponibles para entrenar.\".format(df_train.shape[0]))\n",
        "print(\"Hay {} ejemplos disponibles para predecir.\".format(df_test.shape[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hay 86821 ejemplos disponibles para entrenar.\n",
            "Hay 11873 ejemplos disponibles para predecir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yOYdNNdoQTms",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Parte b)**"
      ]
    },
    {
      "metadata": {
        "id": "4T4ZS74vfG59",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A contiuación, realizamos un prepocesamiento de los textos de entrada y de salida."
      ]
    },
    {
      "metadata": {
        "id": "qFPQ0cjKRQf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0378e94c-89cb-404c-8627-23d7ba635368"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "mgK2avHUQLs6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "banned_tokens = [',','.','?', '!', ';', '``', \"''\", '(', ')']\n",
        "\n",
        "train_questions  = []\n",
        "for sentence in df_train[\"question\"]:\n",
        "  processed_sentence = []\n",
        "  for element in word_tokenize(sentence.lower()):\n",
        "    if not element in banned_tokens: processed_sentence.append(element)\n",
        "  train_questions.append(processed_sentence)\n",
        "\n",
        "test_questions  = []\n",
        "for sentence in df_test[\"question\"]:\n",
        "  processed_sentence = []\n",
        "  for element in word_tokenize(sentence.lower()):\n",
        "    if not element in banned_tokens: processed_sentence.append(element)\n",
        "  test_questions.append(processed_sentence)\n",
        "  \n",
        "\n",
        "train_answers   = [word_tokenize(sentence) for sentence in df_train[\"answer\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "avMPsSrsfNrk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Los textos de entrada (preguntas), tanto de entrenamiento como de prueba, son preprocesados pasándolos a minúsculas y tokenizando esto último, además de filtrar una serie de caracteres que podrían perjudicar las predicciones de la red, como signos de interrogación o exclamación, signos de puntuación, etc. Por otro lado, las respuestas de entrenamiento son preprocesadas sólo tokenizando. El proceso de tokenizado convierte un texto de entrada en un arreglo de palabras, además de separar los caracteres especiales (como los signos de interrogación)."
      ]
    },
    {
      "metadata": {
        "id": "FgVk_ioeSzdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "01ee3a4f-f5f4-4efd-ad61-6d75ee0a300d"
      },
      "cell_type": "code",
      "source": [
        "print(train_questions[7771])\n",
        "print(df_train[\"question\"][7771])\n",
        "'``' in [',','.','?','``', \"''\"]"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['who', 'described', 'the', 'dutch', 'confederacy', 'as', 'exhibiting', 'imbecility', 'in', 'the', 'government', 'discord', 'among', 'the', 'provinces', 'foreign', 'influence', 'and', 'indignities', 'a', 'precarious', 'existence', 'in', 'peace', 'and', 'peculiar', 'calamities', 'from', 'war']\n",
            "Who described the Dutch confederacy as exhibiting \"Imbecility in the government; discord among the provinces; foreign influence and indignities; a precarious existence in peace, and peculiar calamities from war.\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "metadata": {
        "id": "EXANRFgTgdx5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4b2c17a-7166-465f-aab5-d3abdc695cbf"
      },
      "cell_type": "code",
      "source": [
        "print(train_answers[7771])"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['James', 'Madison']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ls87rjLlRaQG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Parte c)**"
      ]
    },
    {
      "metadata": {
        "id": "Y2JTdBHZo2gm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ahora, creamos un vocabulario para codificar las plaabras en las respuestas a generar por nuestra red. Adicionalmente, agregamos el símbolo \"#end\" para delimitar el fin de la palabra, tanto para preguntas como para respuestas."
      ]
    },
    {
      "metadata": {
        "id": "QcPGpJvjRLhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8793729-64b1-4b34-c11e-c4163c239cec"
      },
      "cell_type": "code",
      "source": [
        "#Generamos el diccionario para respuestas\n",
        "vocab_answer = set()\n",
        "for sentence in train_answers:\n",
        "    for word in sentence:\n",
        "        vocab_answer.add(word)\n",
        "vocab_answer = [\"#end\"]+ list(vocab_answer)\n",
        "print('posibles palabras para respuestas :', len(vocab_answer))\n",
        "vocabA_indices = {c: i for i, c in enumerate(vocab_answer)}\n",
        "indices_vocabA = {i: c for i, c in enumerate(vocab_answer)}"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "posibles palabras para respuestas : 47423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oMhYWjqnT8w3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e175322-45d3-428f-8d0c-4e7cab86da8e"
      },
      "cell_type": "code",
      "source": [
        "#Generamos el diccionario para preguntas\n",
        "vocab_question = set()\n",
        "for sentence in train_questions:\n",
        "    for word in sentence:\n",
        "        vocab_question.add(word)\n",
        "for sentence in test_questions:\n",
        "    for word in sentence:\n",
        "        vocab_question.add(word)\n",
        "vocab_question = [\"#end\"]+ list(vocab_question)\n",
        "print('posibles palabras para preguntas :', len(vocab_question))\n",
        "vocabQ_indices = {c: i for i, c in enumerate(vocab_question)}\n",
        "indices_vocabQ = {i: c for i, c in enumerate(vocab_question)}"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "posibles palabras para preguntas : 42051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yhToaKkHq_Dm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Basados en la premisa de que (generalmente) mientras más datos se tengan disponibles mejor, dado que esto previene el overfitting, no debiesemos esperar mayores problemas por parte de la red para realizar las predicciones. Sin embargo, esta gran cantidad de datos sobre los cuales se deberá realizar las predicciones podría significar que los tiempos de entrenamiento de la red podrían ser considerables."
      ]
    },
    {
      "metadata": {
        "id": "dUoQMmeVUQrR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Parte d)**"
      ]
    },
    {
      "metadata": {
        "id": "9A9rwCwd7QFC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A continuación codificamos los tokens de cada texto a utilizar, convirtiendo los sets de preguntas y respuestas a one-hot vectors."
      ]
    },
    {
      "metadata": {
        "id": "zTHzYrr6T5Kj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#input and output to onehotvector\n",
        "X_answers = [[vocabA_indices[palabra] for palabra in sentence] for sentence in train_answers]\n",
        "Xtrain_question = [[vocabQ_indices[palabra] for palabra in sentence] for sentence in train_questions]#same for train question\n",
        "Xtest_question = [[vocabQ_indices[palabra] for palabra in sentence] for sentence in test_questions]#same for test question"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nhAfn3I98B8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ahora realizamos padding sobre los datos sobre todas las secuencias (entrada/salida de entrenamiento, entrada de prueba), para que todas las secuencias tengan las mismas dimensiones y que, de esta forma, puedan ser utilizadas por nuestra red."
      ]
    },
    {
      "metadata": {
        "id": "wVJ4TmhdW-b-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "max_input_lenght  = np.max(list(map(len, train_questions)))\n",
        "max_output_lenght = np.max(list(map(len, train_answers)))+1\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "Xtrain_question = sequence.pad_sequences(Xtrain_question, maxlen=max_input_lenght, padding='post', value=vocabQ_indices[\"#end\"])\n",
        "Xtest_question  = sequence.pad_sequences(Xtest_question, maxlen=max_input_lenght, padding='post', value=vocabQ_indices[\"#end\"])\n",
        "X_answers       = sequence.pad_sequences(X_answers, maxlen=max_output_lenght, padding='post', value=vocabA_indices[\"#end\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H0Oz1snNYpNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a77f7018-d35a-420c-ab41-dabf8320281f"
      },
      "cell_type": "code",
      "source": [
        "print(\"Dimensionalidad Xtrain_question:\",Xtrain_question.shape)\n",
        "print(\"Dimensionalidad X_answers:\\t\",X_answers.shape)\n",
        "print(\"Dimensionalidad Xtest_question:\\t\",Xtest_question.shape)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensionalidad Xtrain_question: (86821, 40)\n",
            "Dimensionalidad X_answers:\t (86821, 47)\n",
            "Dimensionalidad Xtest_question:\t (11873, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "avpMXriU_PZI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "La primera dimensión corresponde a la cantidad de secuencias correspondientes, 86821 para las preguntas+respuestas de entrenamiento y 11873 para las preguntas de prueba. La segunda dimensión corresponde al máximo largo posible de esa secuencia en particular, 60 para el caso de las preguntas (tanto de entrenamiento como de prueba), y 47 para las respuestas."
      ]
    },
    {
      "metadata": {
        "id": "grFMr9cObv5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Parte e)**"
      ]
    },
    {
      "metadata": {
        "id": "AYubOWX5AJTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A continuación definimos nuestro modelo encoder-decoder junto a sus módulos de atención."
      ]
    },
    {
      "metadata": {
        "id": "O-ADZ654a0yJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Encoder-Decoder modelo\n",
        "from keras.layers import Input,RepeatVector,TimeDistributed,Dense,Embedding,Flatten,Activation,Permute,Lambda,CuDNNLSTM,Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "from keras import backend as K\n",
        "\n",
        "lenght_output = max_output_lenght\n",
        "hidden_dim = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DGgk0dlYBLsw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "En esta ocasión utilizaremos una compuerta LSTM en el enconder, optimizada para CUDA de nVidia. Además utilizamos en el encoder una red bidireccional, dado que su uso aumenta el desempeño de la red aunque no por un margen muy amplio, tal como pudimos ver en la tarea anterior."
      ]
    },
    {
      "metadata": {
        "id": "n3IdcK2eb5kN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_vector = 64 \n",
        "encoder_input = Input(shape=(max_input_lenght,))\n",
        "embedded = Embedding(input_dim=len(vocabQ_indices),output_dim=embedding_vector,input_length=max_input_lenght)(encoder_input)\n",
        "encoder = Bidirectional(CuDNNLSTM(hidden_dim, return_sequences=True), merge_mode='concat')(embedded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9YJpws7G2tM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ahora definimos la atención $\\alpha$ que se calculará sobre cada instante de tiempo $T$, computando su atención por cada instante de tiempo de la decodifiación $T'$."
      ]
    },
    {
      "metadata": {
        "id": "QfS14vcYcVTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compute T' importance for each step T\n",
        "attention = TimeDistributed(Dense(max_output_lenght, activation='tanh'))(encoder)\n",
        "#softmax a las antenciones sobre todo T\n",
        "attention = Permute([2, 1])(attention)\n",
        "attention = Activation('softmax')(attention) \n",
        "attention = Permute([2, 1])(attention)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WC2eeBkmKdWw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Aplicamos la atención sobre el encoder, y generamos las salidas:"
      ]
    },
    {
      "metadata": {
        "id": "ynT3ya3Nce3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# apply the attention to encoder\n",
        "def attention_multiply(vects):\n",
        "    encoder, attention = vects\n",
        "    return K.batch_dot(attention,encoder, axes=1)\n",
        "sent_representation = Lambda(attention_multiply)([encoder, attention])\n",
        "decoder = CuDNNLSTM(hidden_dim, return_sequences=True)(sent_representation)\n",
        "probabilities = TimeDistributed(Dense(len(vocab_answer), activation=\"softmax\"))(decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rh1OgqmEKjvf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finalmente, generamos la descripción del modelo:"
      ]
    },
    {
      "metadata": {
        "id": "w8miyqsUcot1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "be799561-e086-4047-92e0-9adae726aece"
      },
      "cell_type": "code",
      "source": [
        "model = Model(encoder_input,probabilities)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 40)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 40, 64)       2691264     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 40, 256)      198656      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 40, 47)       12079       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "permute_3 (Permute)             (None, 47, 40)       0           time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 47, 40)       0           permute_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "permute_4 (Permute)             (None, 40, 47)       0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 47, 256)      0           bidirectional_2[0][0]            \n",
            "                                                                 permute_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_4 (CuDNNLSTM)        (None, 47, 128)      197632      lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 47, 47423)    6117567     cu_dnnlstm_4[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 9,217,198\n",
            "Trainable params: 9,217,198\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i-vitlDDNxth",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Primero, los datos pasan por una capa de *embedding* y posteriormente a la capa de encoder, que en este caso corresponde a una LSTM bidireccional.\n",
        "\n",
        "Posteriormente se computa la atención $\\alpha$ para cada instante $T$ de tiempo, lo que se realiza mediante las capas *TimeDistributed* (la cual aplica una capa a cada paso temporal de una entrada) , *Permute* (que permuta las dimensiones de la entrada de acuerdo a un patrón determinado) y *Activation*  (que aplica una función de activación a la entrada) del modelo.\n",
        "\n",
        "Finalmente se aplica esta atención sobre el encoder, y se generan las salidas mediante la capa de decoder (en este caso, una capa LSTM + una capa *TimeDistributed*)."
      ]
    },
    {
      "metadata": {
        "id": "WXC42-QAc6sY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Parte f)**"
      ]
    },
    {
      "metadata": {
        "id": "gL6wvZa1c6ix",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Procedemos a entrenar el modelo planteado en el punto anteror por 10 epochs, con un tamaño de batch de 128."
      ]
    },
    {
      "metadata": {
        "id": "jzMX0vCWc3vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "d98f797d-d738-4533-fbe2-a17d8613e056"
      },
      "cell_type": "code",
      "source": [
        "X_answers = X_answers.reshape(X_answers.shape[0],X_answers.shape[1],1)\n",
        "print(X_answers.shape)\n",
        "model.fit(Xtrain_question,X_answers,epochs=10,batch_size=128,validation_split=0.2)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(86821, 47, 1)\n",
            "Train on 69456 samples, validate on 17365 samples\n",
            "Epoch 1/10\n",
            "69456/69456 [==============================] - 353s 5ms/step - loss: 1.2077 - val_loss: 0.7955\n",
            "Epoch 2/10\n",
            "69456/69456 [==============================] - 352s 5ms/step - loss: 0.6979 - val_loss: 0.7917\n",
            "Epoch 3/10\n",
            "69456/69456 [==============================] - 351s 5ms/step - loss: 0.6773 - val_loss: 0.8042\n",
            "Epoch 4/10\n",
            "69456/69456 [==============================] - 350s 5ms/step - loss: 0.6617 - val_loss: 0.8036\n",
            "Epoch 5/10\n",
            "69456/69456 [==============================] - 350s 5ms/step - loss: 0.6496 - val_loss: 0.8108\n",
            "Epoch 6/10\n",
            "69456/69456 [==============================] - 350s 5ms/step - loss: 0.6391 - val_loss: 0.8155\n",
            "Epoch 7/10\n",
            "69456/69456 [==============================] - 350s 5ms/step - loss: 0.6293 - val_loss: 0.8252\n",
            "Epoch 8/10\n",
            "69456/69456 [==============================] - 349s 5ms/step - loss: 0.6224 - val_loss: 0.8236\n",
            "Epoch 9/10\n",
            "69456/69456 [==============================] - 349s 5ms/step - loss: 0.6122 - val_loss: 0.8343\n",
            "Epoch 10/10\n",
            "69456/69456 [==============================] - 349s 5ms/step - loss: 0.6058 - val_loss: 0.8309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe0218f1978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "metadata": {
        "id": "SDz_umSedAR0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save(\"modeloT3.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c0Aeg6FVtDBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "240ec9cb-6167-4073-9a29-578947f7cd08"
      },
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 111M\r\n",
            "-rw-r--r-- 1 root root 4.2M Aug 28 05:12 dev-v2.0.json\r\n",
            "-rw-r--r-- 1 root root  11K Aug 30 02:25 evaluate-v2.0.py\r\n",
            "-rw-r--r-- 1 root root 106M Aug 30 05:29 modeloT3.h5\r\n",
            "-rw-r--r-- 1 root root 453K Aug 30 02:42 predictions\r\n",
            "drwxr-xr-x 2 root root 4.0K Aug 29 00:23 sample_data\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7daFRpd5Acbt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Parte g)**"
      ]
    },
    {
      "metadata": {
        "id": "WBpc7DHlBTbk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A contiuación, mosramos ejemplos de la predicción del modelo. Para esto creamos una función que prediga a través de la distribución de probabilidad de la salida, cada palabra en cada instante de tiempo."
      ]
    },
    {
      "metadata": {
        "id": "lr_BKmpiBf1A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import choice\n",
        "\n",
        "def predict_words(model,example,diversity=2):\n",
        "    #predict example\n",
        "    result_model = model.predict(np.reshape(example, (1, 40)))[0]\n",
        "    output = []\n",
        "    for element in result_model:\n",
        "      #print(element)\n",
        "      filtered_elements = np.argsort(element)[-diversity:]\n",
        "      output.append(choice(filtered_elements))\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbqhvTELuiMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "a55acb18-5026-4cf0-8677-4a34c111ef33"
      },
      "cell_type": "code",
      "source": [
        "n=10\n",
        "for i in range(n):\n",
        "    indexs = np.random.randint(0,len(Xtest_question))\n",
        "    example = Xtest_question[indexs]\n",
        "    indexes_answer = predict_words(model,example,1)\n",
        "    question = df_test[\"question\"][indexs]\n",
        "    print(\"Pregunta {0}: {1}\".format(indexs, question))\n",
        "    answer = \"\"\n",
        "    for index in indexes_answer:\n",
        "        if indices_vocabA[index]==\"#end\": # el final de la oracion\n",
        "            continue\n",
        "        else:\n",
        "            answer+=indices_vocabA[index]+\" \"\n",
        "    print(\"Respuesta: \",answer)\n",
        "print(\"Los ha predecido todos!\")"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pregunta 2847: When was the National Highway Designated Act signed?\n",
            "Respuesta:  2006 \n",
            "Pregunta 6681: What P or was damaged during the 2008 tropical storm Fay?\n",
            "Respuesta:  the \n",
            "Pregunta 7745:  How long after a banquet with Tugh Temur did Kusala have a child?\n",
            "Respuesta:  $ \n",
            "Pregunta 4377: What is included with each packet label\n",
            "Respuesta:  the \n",
            "Pregunta 8720: What theorem states that the probability that a number n is prime is inversely proportional to its logarithm?\n",
            "Respuesta:  $ \n",
            "Pregunta 9598: How many members in the seats of the Scottish Parliament are members of the Scottish Government?\n",
            "Respuesta:  two \n",
            "Pregunta 6533: How many Marine bases are located in Jacksonville?\n",
            "Respuesta:  the , , the \n",
            "Pregunta 11140: What was the Seven Years War?\n",
            "Respuesta:  the , the the the \n",
            "Pregunta 1208: How many Victorians are Muslim?\n",
            "Respuesta:  two \n",
            "Pregunta 9144: What is the legal boundary behind the High and Upper Rind?\n",
            "Respuesta:  the \n",
            "Los ha predecido todos!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ImwoTm1WVnW3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Como se puede observar de los respuestas generadas a las preguntas, la red no logra responder todas las preguntas de forma correcta, aunque en varias de las preguntas la red es capaz de proveer una respuesta coherente con el contexto de la pregunta (esto es, la respuesta es un año si es que la pregunta hace referencia a un año, o un número si es que la pregunta hace referencia a una cantidad). Además, hay algunos caracteres adicionales que hicieron su aparición en las respuestas y que no consideramos al momento de tokenizar las preguntas, como el signo peso \"$\" y la coma con un espacio \", \"."
      ]
    },
    {
      "metadata": {
        "id": "yDyM6xnNp2vG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Parte h)**"
      ]
    },
    {
      "metadata": {
        "id": "EhAyHaI0VmeB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ahora, calcularemos el desempeño de nuestra red utilizando el script de evaluación provisto por el proyecto SQuAD, el que además nos permitirá compararnos con otras implementaciones."
      ]
    },
    {
      "metadata": {
        "id": "Z0muW2HPvOan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a4487a2a-7c4d-470d-b318-ac1154dca57a"
      },
      "cell_type": "code",
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-08-30 02:50:16--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2018-08-30 02:50:16 (89.5 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jPmYWmJyXHRT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Primero, realizamos una predicción de la respuesta de todas las preguntas del set de pruebas."
      ]
    },
    {
      "metadata": {
        "id": "KBAze00YBdxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "47e9831c-91e0-4f00-f46b-d5512b1b0783"
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "dic_predictions = {}\n",
        "contador = 1\n",
        "for example,id_e in zip(Xtest_question,df_test[\"id\"]): #todos los ejemplos\n",
        "    print (\"\\r\", \"Ejemplo {0}...\".format(contador), end=\"\")\n",
        "    indexes_answer = predict_words(model,example) #predice palabra en cada instante\n",
        "    answer = \"\"\n",
        "    for index in indexes_answer:\n",
        "        if indices_vocabA[index]==\"#end\": # el final de la oracion\n",
        "            continue\n",
        "        else:\n",
        "            answer+=indices_vocabA[index]+\" \"\n",
        "    dic_predictions[id_e] = answer\n",
        "    contador += 1\n",
        "print(\"\\n\", \"Los ha predecido todos!\")\n",
        "json_save = json.dumps(dic_predictions)\n",
        "archivo = open(\"predictions\",\"w\")\n",
        "archivo.write(json_save)\n",
        "archivo.close()\n"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Ejemplo 11873...\n",
            " Los ha predecido todos!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "we93Ta8tXUTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A continuación, corremos el script de evaluación con nuestras predicciones y el set de pruebas de SQuAD."
      ]
    },
    {
      "metadata": {
        "id": "thfU8LEAp6kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3be970bf-ed87-4142-8acb-2a1358d364fc"
      },
      "cell_type": "code",
      "source": [
        "#evaluar resultados\n",
        "!python evaluate-v2.0.py dev-v2.0.json predictions"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\r\n",
            "  \"exact\": 6.636907268592605,\r\n",
            "  \"f1\": 8.216702714512236,\r\n",
            "  \"total\": 11873,\r\n",
            "  \"HasAns_exact\": 0.10121457489878542,\r\n",
            "  \"HasAns_f1\": 3.265335919265187,\r\n",
            "  \"HasAns_total\": 5928,\r\n",
            "  \"NoAns_exact\": 13.153910849453322,\r\n",
            "  \"NoAns_f1\": 13.153910849453322,\r\n",
            "  \"NoAns_total\": 5945\r\n",
            "}\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "396Z308MfmtN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Como podemos observar, el desempeño de nuestra red es bastante bajo en comparación al leaderboard presentado en https://rajpurkar.github.io/SQuAD-explorer/, siendo más de 7 veces peor que la peor red presentada en este ranking (según indican los valores *exact* y *f1*). Esto puede deberse a un pobre preprocesado de los datos de entrenamiento, haciendo referencia a los caracteres especiales que aparecieron en las predicciones del punto anterior, así como por el relativamente simple hecho de que nuestra red es demasiado siemple para un problema tan complejo como lo es el procesado de lenguaje natural. La complejidad de este dataset en particular radica en que  no provee una lista de respuestas posibles, y en su lugar obliga al modelo a que escoja entre cualquier palabra posible en el contexto de la pregunta. Lo anterior nos sugiere que nuestro modelo no fue capaz de predecir una respuesta que se ajustara al contexto de la pregunta original, algo que puede ser mejorado cambiando la arquitectura de la red o mejorando los hiperparámetros de la red original."
      ]
    },
    {
      "metadata": {
        "id": "yFnsEy4bf_L9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}